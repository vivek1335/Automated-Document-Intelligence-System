# -*- coding: utf-8 -*-
"""automated_document_intelligence

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ztWU1KHzdJqfLMxvLckPWx46G2lkQsgs
"""

# automated_document_intelligence.py

import os
import pandas as pd
import spacy
import nltk
from nltk.corpus import stopwords
from PyPDF2 import PdfReader
from docx import Document
from gensim.summarization import summarize
from collections import Counter

nltk.download('stopwords')

nlp = spacy.load("en_core_web_sm")
stop_words = set(stopwords.words('english'))

# ----------------------------
# 1. Load Document
# ----------------------------
def load_document(file_path):
    text = ""
    if file_path.endswith('.pdf'):
        reader = PdfReader(file_path)
        for page in reader.pages:
            text += page.extract_text() + "\n"
    elif file_path.endswith('.docx'):
        doc = Document(file_path)
        for para in doc.paragraphs:
            text += para.text + "\n"
    elif file_path.endswith('.txt'):
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
    else:
        raise ValueError("Unsupported file type. Use PDF, DOCX, or TXT.")
    return text

file_path = input("Enter path to the document: ")
document_text = load_document(file_path)
document_text = document_text.replace("\n", " ")

# ----------------------------
# 2. Keyword Extraction
# ----------------------------
def extract_keywords(text, top_n=10):
    words = [word.lower() for word in text.split() if word.isalpha() and word.lower() not in stop_words]
    common_words = Counter(words).most_common(top_n)
    return [word for word, _ in common_words]

keywords = extract_keywords(document_text, top_n=10)

# ----------------------------
# 3. Named Entity Recognition (NER)
# ----------------------------
doc = nlp(document_text)
entities = [(ent.text, ent.label_) for ent in doc.ents]

# ----------------------------
# 4. Summarization
# ----------------------------
try:
    summary = summarize(document_text, word_count=100)
except:
    summary = "Document too short to summarize."

# ----------------------------
# 5. Sentiment Analysis
# ----------------------------
from nltk.sentiment import SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()
sentiment_score = sia.polarity_scores(document_text)
sentiment = max(sentiment_score, key=lambda k: sentiment_score[k] if k != 'compound' else 0)

# ----------------------------
# 6. Display Results
# ----------------------------
print("\n--- Document Insights ---")
print(f"Keywords: {keywords}")
print(f"Named Entities: {entities[:10]}")  # show top 10
print(f"Summary: {summary}")
print(f"Sentiment: {sentiment}")

# ----------------------------
# 7. Export to CSV
# ----------------------------
output_file = "document_insights.csv"
df = pd.DataFrame({
    "Keywords": [', '.join(keywords)],
    "Named Entities": [', '.join([f"{text}({label})" for text, label in entities[:10]])],
    "Summary": [summary],
    "Sentiment": [sentiment]
})
df.to_csv(output_file, index=False)
print(f"\nInsights saved to {output_file}")